---
title: "Participants"
weight: 7
header_menu: true
draft: false
---

{{< figure class="clear" >}}

##### Abdallah El Ali

{{< figure src="https://abdoelali.com/images/home_imgs/photo_abdallah_el_ali.jpg" width="200" alt="Abdallah El Ali" class="left" >}}

Dr. Abdallah El Ali is a research scientist in Human Computer Interaction at Centrum Wiskunde & Informatica (CWI) in Amsterdam. He leads the research area on Affective Interactive Systems, where he combines advances in Human-Computer Interaction, eXtended Reality, and Artificial Intelligence to measure, infer, and augment human cognitive, affective, and social interactions. He is also on the executive board for CHI Nederland (CHI NL), the Dutch ACM SIGCHI Local Chapter. {{<extlink text="Personal website." href="https://abdoelali.com" icon="fa fa-external-link">}}

Talk title: ***Affective Augmentation Systems for Mind, Body, and Social Connection***

<!-- Talk abstract: We are entering a digital wave where the user experience is transforming through immersive, interactive, and multi-sensory technologies. Advancements in biosensing and actuation enable us to not only visualize hidden physiological data, but also to create artificial haptic sensations (thermal, vibrotactile), which together can potentially enhance our mind, body, and social connections. This creates unfamiliar human-human and human-machine interactions that warrant further exploration. In this talk, I will introduce what we call Affective Augmentation Systems: systems designed to enhance, modify, or diminish our physical/virtual bodies and senses, creating new affective user experiences. Through several research prototypes, I will explore what effects affective augmentation systems can have on user experience across the reality-virtuality spectrum. I will conclude with a cautionary outlook, emphasizing the need for designing *responsible* human-machine integration technology. -->


{{< figure class="clear" >}}
##### Alexander Marquardt

{{< figure src="images/Alex.jpeg" width="200" alt="Alexander Marquardt" class="right" >}}

Dr. Alexander Marquardt is a postdoctoral researcher at the Institute for Visual Computing, Department of Computer Science, Bonn-Rhein-Sieg University of Applied Sciences (BRSU), Germany.

Talk title: ***Towards Awe and Beyond: The Role of Multisensory Cues in Affective Wellbeing***

<!-- Talk abstract: This talk explores the utilization of multisensory cues to enhance user interaction and elicit affective responses. Integrating various sensory stimuli such as visual, auditory, and tactile, along with other potential sensory stimuli, enriches user engagement in immersive environments. This venture into multisensory cue integration opens up a promising avenue for understanding and enhancing affective wellbeing. Examples range from supporting anxiety treatment to exploring complex emotional states such as awe in VR, showcasing the diverse potential of multisensory-enhanced environments. The discussion aims to foster a multidisciplinary dialogue on leveraging multisensory cues to cultivate a positive impact on well-being. -->


{{< figure class="clear" >}}
##### Jacqueline Bailey

{{< figure src="images/Jacqueline.jpeg" width="200" alt="Jacqueline Bailey" class="left" >}}

Dr. Jacqueline Bailey is an Associate Lecturer in HCI and HCC at the University of Newcastle, Australia. She obtained a PhD in Information Technology and Bachelor of Information Technology with First Class Honours from the University of Newcastle. She works in partnership with the i3Lab mainly with avatars and agents in Simulation Training and Serious Games. Her research focuses on the quantitative analysis of human perceptions of avatars, and in particular, the use of the human startle reflex as a measure of affective processing. Previously, she worked collaboratively with the Australian Defence College Simulation Centre to evaluate how emotional expressions in avatars’ of differing levels of realism and fidelity are perceived by human users.

Talk title: ***The i3Lab: Industry linked research***

<!-- Talk abstract: This talk explores the utilization of multisensory cues to enhance user interaction and elicit affective responses. Integrating various sensory stimuli such as visual, auditory, and tactile, along with other potential sensory stimuli, enriches user engagement in immersive environments. This venture into multisensory cue integration opens up a promising avenue for understanding and enhancing affective wellbeing. Examples range from supporting anxiety treatment to exploring complex emotional states such as awe in VR, showcasing the diverse potential of multisensory-enhanced environments. The discussion aims to foster a multidisciplinary dialogue on leveraging multisensory cues to cultivate a positive impact on well-being. -->


{{< figure class="clear" >}}
##### Katsutoshi Masai

{{< figure src="images/Masai.jpeg" width="200" alt="Katsutoshi Masai" class="right" >}}

Dr. Katsutoshi Masai received the Ph.D. degree from Keio University, Japan, in 2018, He worked as a JSPS Research Fellow (DC2), a Project Research Associate, a Research Associate at Keio University, Research Associate at Japanese company, He is currently Assistant Professor at Kyushu University. His research interest includes wearable interfaces for sensing human daily activities.

Talk title: ***Smart eyewear for facial expression recognition and interaction***

<!-- Talk abstract: This presentation will explore the innovative world of smart eyewear using photo reflective sensors, designed to recognize facial expressions in real-life situations. As facial expressions are crucial for understanding a person's emotions and thoughts, it is essential to develop effective recognition technologies that can seamlessly integrate into daily life. The photo reflective sensing approach enables the detection of even the subtle facial expressions. In addition, I will introduce the various applications.  -->

{{< figure class="clear" >}}
##### Kevin El Haddad  

{{< figure src="images/Kevin.jpeg" width="200" alt="Kevin El Haddad" class="left" >}}

Kevin El Haddad  is a researcher at the ISIA Lab of University of Mons in Belgium where he works on human-agent interaction applications in general with specific focus on industrial applications and nonverbal expression processing. At the same time, he leads the R&D department at Diabolocom (Paris, France).


{{< figure class="clear" >}}
##### Kongmeng Liew

{{< figure src="images/Kong.jpeg" width="200" alt="Kongmeng Liew" class="right" >}}

Dr. Kongmeng Liew is a cultural psychologist, and lecturer (assistant professor) in psychology of social media and emerging online technologies at the University of Canterbury in New Zealand. His research uses computational methods to examine social psychological topics, focusing on issues like cultural marginalisation, cultural differences in affect and emotion, and the emergence of online culture.

Talk title: ***Groovin’to the cultural beat: Preferences for danceable music represent cultural affordances for high-arousal negative emotions***

<!-- Talk abstract: This talk will be introducing how Spotify's catalogue of music features for their music database can track listeners' emotions, both at an individual-level, and on a macro-level. These cross-cultural differences in music consumption, tracked through these music features, can be used to represent socio-cultural differences in societal attitudes towards affect and emotion.  -->


{{< figure class="clear" >}}
##### Mayra Barrera

{{< figure src="images/Mayra.png" width="200" alt="Mayra Barrera" class="left" >}}

Dr. Mayra Donaji Barrera Machuca is an assistant professor at the Faculty of Computer Science, Dalhousie University. She leads the VERTEX lab, which works on understanding people's thinking when working inside a 3D virtual environment and 3DUI design. Her current areas of interest are 3D sketching in VR, skills transfer between VR and Real Life, and depth and spatial perception in VR. {{<extlink text="Personal website." href="https://mayradonaji.net" icon="fa fa-external-link">}}

Talk title: ***New Multimodal sensory inputs for 3D Sketching***

<!-- Talk abstract: 3D sketching is a  new art form that allows people to sketch in 3 dimensions using a VR device directly. This characteristic brings advantages like intuitiveness and flexibility. Yet, one big issue is that it removes some of the sensory feedback that sketching with pen and paper provides. In this talk, I will discuss two projects to enhance the 3D sketching experience using vibrotactile textures and thermal feedback.  -->

{{< figure class="clear" >}}
##### Melissa Steininger

{{< figure src="images/Melissa.jpeg" width="200" alt="Melissa Steininger" class="right" >}}

Melissa Steininger is a research assistant at the Institute for Visual Computing, Department of Computer Science, Bonn-Rhein-Sieg University of Applied Sciences (BRSU), Germany. She recently completed my Master's degree in Visual Computing at BRSU and will soon start her PhD. She has been working at the BRSU for three years, where she supported research on improving interaction in AR, created immersive environments and integrated biosignal measurements into experiments. Her interest lies in the combination of purpose-driven causes, such as promoting well-being as well as enhancing intercultural understanding.

Talk title: ***Affective responses in immersive technologies***

<!-- Talk abstract: In this talk, I will share insights from projects that utilize biosignal analysis to explore affective responses in Augmented and Virtual Reality environments. Initially, I will discuss a project that measures stress levels during AR usage. Following this, I will present how stimuli for eliciting awe are validated in VR. These explorations contribute to a broader understanding of emotional engagement in VR, creating new possibilities for future research on eliciting diverse emotions and enhancing affective well-being through immersive technologies. Future discussions will touch on eliciting diverse emotions in VR and exploring the linguistic relativity of emotions across various cultures.  -->


{{< figure class="clear" >}}
##### Miao Cheng

{{< figure src="images/Miao.jpeg" width="200" alt="Miao Cheng" class="left" >}}

Dr. Miao Cheng is an assistant professor at the Research Institute of Electrical Communication of Tohoku University (Sendai, Japan).


{{< figure class="clear" >}}
##### Michael Knierim

{{< figure src="images/Michael.jpeg" width="200" alt="Michael Knierim" class="right" >}}

Dr. Michael Knierim is a post-doctoral researcher in the Karlsruhe Institute of Technology (KIT), in Germany - in a region they like to call the LÄND. He has a study background in business and economics and has ventured into industrial engineering and computer science during his PhD. His research focuses on creating knowledge and technologies that improve productivity and well-being in everyday work life. Specifically, He is currently researching the potentials of (1) EEG headphone sensors, and (2) how various sensors can be used to detect and support the so-called flow experience.

Talk title: ***Open-source hardware for multimodal biosignal collection from headphones***

<!--
Talk abstract:  I would like to share the latest developments in my earables research. It focuses on further advancing open-source hardware for multimodal biosignal collection from the ear region in the convenient form factor of over ear headphones (https://github.com/MKnierim/openbci-headphones). During the conference, I’ll highlight our latest design advances and how we plan to study affective experiences and well-being with it in everyday life.-->

{{< figure class="clear" >}}
##### Monica Perusquia-Hernandez

{{< figure src="images/Mon.jpg" width="200" alt="Monica Perusquia-Hernandez" class="left" >}}

Dr. Monica Perusquia-Hernandez is an assistant professor at the Nara Institute of Science and Technology (NAIST), in Japan. She is interested in affective computing, bio-signal processing, and interoceptive awareness enhancement using cyber-physical systems. In particular, she works with Computer Vision, Electromyography, Electroencephalography, Electrocardiography, and Skin Conductance for congruence estimation between facial expressions and emotions when assessing subjective user experience, time perception, and affective awareness.

Talk title: ***Embodied correlates of subjective experiences***

<!--
Talk abstract:  -->


{{< figure class="clear" >}}
##### Naoya Zushi

{{< figure src="images/Naoya.jpg" width="200" alt="Naoya Zushi" class="right" >}}

Naoya Zushi is a a graduate student in the Doctoral Program in Neuroscience, University of Tsukuba, and Research Fellowship for Young Scientists, JSPS. His background is in psychology, and his research interest is the relationship between emotion and perception.

Talk title: ***Influence of emotional state on taste perception***

<!-- Talk abstract: Negative emotions, such as anxiety, suppress the perception of sweetness for physically identical food and drink. Such phenomenon of emotion affecting taste perception has been reported, but the mechanism by which this occurs is unknown. -->


{{< figure class="clear" >}}
##### Nilay Yalcin

{{< figure src="images/Nilay.png" width="200" alt="Nilay Yalcin" class="left" >}}

Dr. Nilay Yalcin is an Assistant Professor at Simon Fraser University with a background in Cognitive Science, AI and HCI. Her research focuses on modeling socio-emotional behaviors in computational systems in order to develop interactive systems that can understand human behavior and advance our understanding of human cognition by providing us means to evaluate our assumptions in a systematic and controlled environment. She has been tackling on complex concepts such as empathy, affect, personality and theory of mind in a variety of contexts such as healthcare, education and creativity. She plans to continue pursuing research to understand factors shaping people’s behaviors with their growing personal interactions with virtual agents and develop innovations that would have a positive effect on our society. She is also a proud member of Women in CS and Women in ML groups, and an advocate of Open Science and Open-Source Software.

Talk title: ***Towards Empathy: Bridging Affective and Cognitive Processes***

<!-- Talk abstract: Computational empathy is a recently emerging research area that builds on the premise of equipping intelligent virtual agents (IVAs) with ability to act empathically, namely being able to recognize, resonate to and respond to the affective state of the users. However, empathy is a very complex phenomenon that requires interaction of low-level (i.e., emotion recognition and expression) and high-level capabilities (e.g., appraisal, theory-of-mind) that result in a wide-range of behaviors (i.e., mimicry, affect matching, consolation, perspective-taking, targeted helping). In this talk, I will briefly summarize the contributions of my lab on modeling computational empathy, the wide variety of use-cases we implemented our empathic agents in and the missing pieces of the puzzle that we hope to figure out with interdisciplinary collaborations. -->


{{< figure class="clear" >}}
##### Panote Siriaraya

{{< figure src="images/Panote.png" width="200" alt="Panote Siriaraya" class="right" >}}

Dr. Panote Siriaraya is an assistant professor at Kyoto Institute of Technology. His field is mainly in HCI, particularly in relation to mental well-being, though lately, he has been doing more work in machine learning and recommender systems. His previous research has been about designing VR systems to help people with dementia, creating gamifications of various cognitive training and cognitive behavioral therapies to make them more engaging, and lately, developing virtual worlds for people who are visually impaired.

Talk title: ***Games and VR technology in healthcare***

<!-- Talk abstract: A range of multidisciplinary research projects related to the use of games and VR technology in healthcare will be discussed. These projects include using gamification to enhance mental healthcare therapies, supporting healthy aging through online 3D virtual environments, using gesture-based interaction and tangible interfaces to augment virtual world interaction and engage people with dementia, and using AR based spatialized 3D sound to create a running application where users can compete against themselves in real-time. The central theme of these projects lies in the theoretical and practical demonstration of how VR and game technology could be applied to solve real-world problems, often by blending the physical and virtual worlds. -->



{{< figure class="clear" >}}
##### Scinob Kuroki

{{< figure src="http://www.kecl.ntt.co.jp/people/kuroki.shinobu/Natchan.jpg" width="200" alt="Scinob Kuroki" class="left" >}}

Dr. Scinob Kuroki is a senior research scientist at NTT Communication Science Laboratories (Tokyo, Japan).
{{<extlink text="Personal website." href="http://www.kecl.ntt.co.jp/people/kuroki.shinobu/index-e.html" icon="fa fa-external-link">}}



{{< figure class="clear" >}}
##### Shigeo Yoshida

{{< figure src="images/Shigeo.jpeg" width="200" alt="Shigeo Yoshida" class="right" >}}

Dr. Shigeo Yoshida is an HCI researcher / interaction designer. He is currently working at OMRON SINIC X Corporation as a senior researcher in Japan. He received his Ph.D. in Information Studies, master’s degree in Arts and Sciences, and bachelor’s degree in Engineering from The University of Tokyo in 2017, 2014, and 2012 respectively. During 2017-2022, he was mainly working at The University of Tokyo ( Cyber Interface Laboratory and Information Somatics Laboratory). His research interest involves a broad area of Human-Computer Interaction. He has been especially focusing on designing interactions based on the mechanisms of perception and cognition of our body.
{{<extlink text="Personal website." href="https://shigeodayo.me" icon="fa fa-external-link">}}

Talk title: ***Designing Emotional Experiences using Computationally Generated Bodily Expressions***

<!-- Talk abstract: Our body and mind are inseparable, and we perceive the real world through various sensory information intrinsic and extrinsic to our bodies. In the research field of human-computer interaction (HCI), I have been exploring a method to achieve changes in perception, cognition, and behavior through properly designing sensory information. In my talk, I will showcase my past works on designing emotional experiences using computationally generated bodily expressions. -->


{{< figure class="clear" >}}
##### Taku Hachisu

{{< figure src="images/Hachisu.jpeg" width="200" alt="Taku Hachisu" class="left" >}}

Dr. Taku Hachisu received a Ph.D. in engineering from the University of Electro-Communications, Japan 2015. He is an assistant professor at the University of Tsukuba, Japan. He was a visiting student at INRIA in Rennes, France, an internship fellow at Microsoft Research in Beijing, China, and a visiting scholar at the University of California, Santa Barbara. His research interests include augmented/virtual reality, haptics, human–computer interactions, and wearable devices.

Talk title: ***EnhancedTouch: Wearable Devices for Augmenting Interpersonal Touch Interactions***

<!-- Talk abstract: I present EnhancedTouch, a novel bracelet-type wearable device for facilitating human-human physical touch. In particular, I aim to support children with autism spectrum disorder (ASD), who often exhibit particular communication patterns, such as lack of physical touch. EnhancedTouch is a unique device that can measure human-human touch events and provide visual and vibration feedback to augment touch interaction. I employ inter-body area network technology for communication with partner devices via modulated electrical current flowing through the users’ hands. The developed device can connect to an external device via Bluetooth Low Energy for monitoring and logging where, when, how long, who, and how the touch interactions occurred. These daily augmented touch interactions provided by such contextual information would offer a variety of applications to facilitate social interactions. -->


{{< figure class="clear" >}}
##### Yoshiko Yabe

{{< figure src="images/yoshiko.jpeg" width="200" alt="Yoshiko Yabe" class="right" >}}

Dr. Yoshiko Yabe is a researcher at the Department of Functional Brain Imaging, National Institutes for Quantum and Radiological Science and Technology (QST) in Chiba, Japan. She started her academic career as a Ph.D. student at the University of Tokyo, Japan. She was working on the effect of body movement (treadmill walk) on visual motion perception. The relationship between body movements and perception/cognition has been one of my key research topics. Afterward, she became an assistant professor at Kochi University of Technology, where she became interested in another long-term research topic: time. During her second postdoc at Western University in Canada, she focused on how action changes timing perception via the striatal dopaminergic system. With her experience in cognitive neuroscience of the relationship between body movement and perception, she investigated cognitive capabilities to remember spatial and temporal information in athletes at NTT Communication Science Laboratories.

Talk title: ***Time perspective and well-being***

<!-- Talk abstract: Currently, at QST, I am working on two projects. One of which is about how young people obtain the temporal perspective of the past, present, and future when they experience a catastrophic situation in which nothing is consistent in time (everything that normally happened yesterday does not happen today and nothing that happened today would happen tomorrow). This study is run on young people who experienced the Great East Japan Earthquake in 2011. I am collaborating with Prof. Sachie Yamada who interviewed survivors in Iwate just after the disaster. I want to show you our preliminary data at the workshop. Another project that I am working on now is the one about how to help individuals be positive and motivated. This study is run by Prof. Makiko Yamada at QST who is a PI of the Moonshot Research & Development Program Goal 9: Increasing peace of mind and vitality. I would like to talk about some of our progress on this project. -->


{{< figure class="clear" >}}
##### Yung-Hao Yang

<!-- {{< figure src="images/Yutaro.jpg" width="200" alt="Yung-Hao Yang" class="left" >}} -->

Dr. Yung-Hao Yang is a researcher affiliated with a specific program in Kyoto University’s Graduate School of Informatics. He attained the Ph.D. degree from the Department of Psychology at National Taiwan University, with a focus on unconscious visual perception, particularly related to aspects like high-level semantics and emotions, by using a range of research methods such as psychophysical techniques, fMRI, ERP, and Eye-tracking systems.
Following his doctoral studies, he undertook the role of a research fellow at Harvard Medical School’s Visual Attention Lab, where he captivated the nature of human attention when exposed to unconventional stimuli, such as instances of instability. Additionally, he collaboratied with a Japanese company, where he conducted research into the effects of auditory information on pupil dilation. Currently, his research is centered on the perception of human motion within naturalistic scenes.

Talk title: ***Perception of human motion within naturalistic scenes***

<!-- Talk abstract: NA -->



{{< figure class="clear" >}}
##### Yutaro Hirao

{{< figure src="images/Yutaro.jpg" width="200" alt="Yutaro Hirao" class="left" >}}

Dr. Yutaro Hirao (平尾悠太朗) is Assistant Professor at Nara Institute of Science and Technology (NAIST), Japan (23-). His main research interests include virtual reality (VR), cross-modal interaction, and haptic perception (pseudo-haptics). He received his B.S. and M.S. in engineering from Waseda University (18-20) in Japan, and his Ph.D. in information science and technology form the University of Tokyo (20-23).

Talk title: ***Pseudo-haptics that endeavors to modify haptic perception by using visual and/or audio stimuli***

<!-- Talk abstract: NA -->


{{< figure class="clear" >}}
##### Wan-Jou (Lavender) She

{{< figure src="images/Lavender.jpeg" width="200" alt="Lavender She" class="right" >}}

Dr. Wan-Jou (Lavender) She is an assistant professor at the Social Computing Lab of the Nara Institute of Science and Technology in Japan. Her background is in HCI, and she has primarily focused on areas related to grief care and mental well-being in the past. Lately, she has been diving into research on medical chatbots and LLM-related studies. {{<extlink text="LinkedIn" href="https://www.linkedin.com/in/lavendershe/" icon="fa fa-external-link">}}

Talk title: ***How can we use AI technologies to promote resilience?***

<!-- Talk abstract: I’ll be sharing insights from some of the projects I’ve worked on that have guided my research in this direction  -->
